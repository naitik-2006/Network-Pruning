{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocoevalcap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcider\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cider\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mget_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_loader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 27 16:57:52 2024\n",
    "\n",
    "@author: jishu\n",
    "\"\"\"\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider import cider\n",
    "\n",
    "from get_loader import get_loader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def caption_generate(model,dataset,image,device,max_length = 50):\n",
    "    outputs=[dataset.vocab.stoi[\"<SOS>\"]]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor =torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "        image = image.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image,trg_tensor)\n",
    "            \n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "        \n",
    "        if best_guess == dataset.vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "    caption = [dataset.vocab.itos[idx] for idx in outputs]\n",
    "    \n",
    "    return caption[1:]\n",
    "\n",
    "def run_validation(model, validation_dataloader, validation_dataset, max_len, device, writer):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    results_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx , (image , caption) in enumerate(validation_dataloader):\n",
    "            \n",
    "            count += 1\n",
    "            image = image.to(device)\n",
    "            #encoder_mask = batch[\"decoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "            \n",
    "            # Check that the batch size is 1\n",
    "            assert image.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            print(\"Processing Image:\", count)\n",
    "            model_out = caption_generate(model, validation_dataset , image , device , max_len)\n",
    "\n",
    "            # Convert PyTorch tensors to NumPy arrays\n",
    "            target_text = caption.detach().cpu().numpy().tolist()\n",
    "            target_text_flat = [token for sublist in target_text for token in sublist]\n",
    "            \n",
    "            # Initialize strings to store the predicted and target text\n",
    "            model_out_text = \"\"\n",
    "            target_text_2 = \"\"\n",
    "\n",
    "            # Iterate over the predicted tokens\n",
    "            for i in model_out:\n",
    "                 \n",
    "                token = i\n",
    "                if token == '<EOS>':\n",
    "                    break\n",
    "                model_out_text += token + \" \"\n",
    "\n",
    "            # Iterate over the target tokens\n",
    "            for i in target_text_flat:\n",
    "                token = validation_dataset.vocab.itos[i]\n",
    "                if token == '<EOS>':\n",
    "                    break\n",
    "                target_text_2 += token + \" \"\n",
    "\n",
    "\n",
    "            expected.append(target_text_2.strip())\n",
    "            predicted.append(model_out_text.strip())\n",
    "            results_dict = {}  # Initialize an empty dictionary\n",
    "\n",
    "            results_dict[target_text_2.strip()] = model_out_text.strip()\n",
    "\n",
    "            # Alternatively, if you have a loop for multiple pairs, you can use:\n",
    "            \n",
    "            print(\"Expected :- \", target_text_2)\n",
    "            print(\"Predicted :- \", model_out_text)\n",
    "            results_dict[idx] = [expected,predicted]\n",
    "        \n",
    "    if writer:\n",
    "        # Compute the ROUGE scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge1, rouge2, rougeL = 0, 0, 0\n",
    "        \n",
    "        cider_scorer = cider.Cider()\n",
    "        ciders = 0\n",
    "        \n",
    "        # Dictionaries to store the ground truths (gts) and predictions (res) for CIDEr scoring\n",
    "        gts = {}\n",
    "        res = {}\n",
    "        \n",
    "        for idx, (ref, pred) in enumerate(zip(expected, predicted)):\n",
    "            # Ensure ref is a list of sentences\n",
    "            if isinstance(ref, str):\n",
    "                ref = [ref]\n",
    "    \n",
    "            # Ensure pred is a list containing a single sentence\n",
    "            if isinstance(pred, str):\n",
    "                pred = [pred]\n",
    "\n",
    "            # Sanity check\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) > 0)\n",
    "            assert(type(pred) is list)\n",
    "            assert(len(pred) == 1)\n",
    "\n",
    "            # Fill the gts and res dictionaries for CIDEr scoring\n",
    "            gts[idx] = ref\n",
    "            res[idx] = pred\n",
    "\n",
    "        for ref, pred in zip(expected, predicted):\n",
    "            \n",
    "            scores = scorer.score(ref, pred)\n",
    "            rouge1 += scores['rouge1'].fmeasure\n",
    "            rouge2 += scores['rouge2'].fmeasure\n",
    "            rougeL += scores['rougeL'].fmeasure\n",
    "            \n",
    "       \n",
    "        # Compute CIDEr score for the entire corpus\n",
    "        cider_score, cider_scores = cider_scorer.compute_score(gts, res)\n",
    "        ciders += cider_score\n",
    "\n",
    "        rouge1 /= len(expected)\n",
    "        rouge2 /= len(expected)\n",
    "        rougeL /= len(expected)\n",
    "\n",
    "        writer.add_scalar('validation ROUGE1', rouge1)\n",
    "        writer.add_scalar('validation ROUGE2', rouge2)\n",
    "        writer.add_scalar('validation ROUGEL', rougeL)\n",
    "        writer.flush()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"ROUGE-1 = \", rouge1)\n",
    "        print(\"ROUGE-2 = \", rouge2)\n",
    "        print(\"ROUGE-L = \", rougeL)\n",
    "        print(\"CIDEr = \", ciders)\n",
    "        \n",
    "    with open(\"results.json\", \"w\") as json_file:\n",
    "        json.dump(results_dict, json_file, indent=4)\n",
    "\n",
    "# The rest of your code, such as data loading and model definition, goes here.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the hyperparameters\n",
    "    transform = transforms.Compose([transforms.Resize((350,350)),\n",
    "                                transforms.RandomCrop((256,256)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    images_path , caption_path = r\"D:\\ML\\Korea\\Jishu\\Jishu\\rsicd\\images\" , r\"D:\\ML\\Korea\\Jishu\\Jishu\\rsicd\\captions.csv\"\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    validation_dataloader , validation_dataset = get_loader(images_path,caption_path ,transform,batch_size = BATCH_SIZE,num_workers=4 , train = False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_epochs = 15\n",
    "    learning_rate = 3e-4\n",
    "    trg_vocab_size = len(validation_dataset.vocab)\n",
    "\n",
    "    embedding_size = 512\n",
    "    num_heads = 8\n",
    "    num_decoder_layers = 4\n",
    "    dropout = 0.20\n",
    "    pad_index=validation_dataset.vocab.stoi[\"<PAD>\"]\n",
    "    save_model = True\n",
    "    writer =SummaryWriter(\"runs/loss_plot\")\n",
    "    step = 0\n",
    "    max_len = 50\n",
    "    \n",
    "    \n",
    "    # Now we load the model\n",
    "    model = torch.load('model.pth')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize the tensorboard\n",
    "    logs_dir = \"logs\"\n",
    "    writer = SummaryWriter(logs_dir)\n",
    "\n",
    "    # Now we have to send these dataset and dataloaders to the run_validation function\n",
    "    \n",
    "    run_validation(model, validation_dataloader, validation_dataset, max_len, device , writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
