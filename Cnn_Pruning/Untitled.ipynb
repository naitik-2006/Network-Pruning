{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2a9efd-44ba-4c62-9b77-0544b888f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\codes\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import Tiny_ImageNet_Loader\n",
    "from torchvision import transforms\n",
    "from get_loader import get_loader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# First define the original model. The pruned model and the original model will be instances of this\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self , embedding_size = 512 , train_CNN = False):\n",
    "        super(EncoderCNN , self).__init__()\n",
    "        self.embedding_size = embedding_size \n",
    "        self.train_CNN = train_CNN\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        \n",
    "        # Remove the classification layer (avgpool and fc) at the end\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules) # Size (B x 2048 x H x W)\n",
    "\n",
    "        # Fully connected layer to transform features to embedding size\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        \n",
    "        # We do not freeze the parameters of the resnet here because we have to prune the model\n",
    "        if not train_CNN:\n",
    "            for params in self.resnet.parameters():\n",
    "                params.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.shape[0] , features.shape[1] , -1)\n",
    "        features = features.permute(2 , 0 , 1)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "    \n",
    "class MySlimmingPruner(tp.pruner.MetaPruner):\n",
    "    def regularize(self , model , reg):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m , (nn.BatchNorm1d , nn.BatchNorm2d , nn.BatchNorm3d)) and m.affine == True:\n",
    "                m.weight.grad.data.add_(reg*torch.sign(m.weight.data))                \n",
    "class MySlimmingImportance(tp.importance.Importance):\n",
    "    def __call__(self , group):\n",
    "        group_imp = []\n",
    "        for dep , idx in group:\n",
    "            layer = dep.target.module\n",
    "            prune_fn = dep.handler\n",
    "            if isinstance(layer, (nn.BatchNorm1d , nn.BatchNorm2d , nn.BatchNorm3d)) and layer.affine:\n",
    "                local_imp = torch.abs(layer.weight.data)\n",
    "                group_imp.append(local_imp)\n",
    "        if(len(group_imp) == 0): return None\n",
    "        \n",
    "        group_imp = torch.stack(group_imp , dim = 0).mean(dim = 0)\n",
    "        return group_imp # This has the dimension(num_channels , _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e33562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43a8ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\codes\\anaconda3\\envs\\ml\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\codes\\anaconda3\\envs\\ml\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_model = EncoderCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cce0c7e-7d93-4adc-86b3-763718148c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pruned_model = EncoderCNN(train_CNN=True).to(device)  # Train_CNN is set to true because we want to change its parameters\n",
    "\n",
    "# Set the importance Criteria\n",
    "imp = MySlimmingImportance()\n",
    "\n",
    "# Define the data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(350),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# Define the paths for images and captions \n",
    "images_path = r\"D:\\ML\\Korea\\Jishu\\Jishu\\rsicd\\images\"\n",
    "caption_path = r\"D:\\ML\\Korea\\Jishu\\Jishu\\rsicd\\captions.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Get the data loader\n",
    "fine_tune_loader, fine_tune_dataset = get_loader(images_path, caption_path, transform)\n",
    "\n",
    "# Define the layers which need to be ignored\n",
    "ignored_layers = []\n",
    "for m in original_model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 512:\n",
    "        ignored_layers.append(m)\n",
    "\n",
    "# Define example input\n",
    "example_inputs = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "# Pruner initialization\n",
    "iterative_steps = 5  # Number of iterations to achieve target pruning ratio\n",
    "pruner = MySlimmingPruner(\n",
    "    pruned_model, \n",
    "    example_inputs, \n",
    "    global_pruning=False,  # If False, a uniform ratio will be assigned to different layers.\n",
    "    importance=imp,  # Importance criterion for parameter selection\n",
    "    iterative_steps=iterative_steps,  # The number of iterations to achieve target ratio\n",
    "    pruning_ratio=0.5,  # Remove 50% channels\n",
    "    ignored_layers=ignored_layers\n",
    ")\n",
    "\n",
    "# Create the directory for saving models if it doesn't exist\n",
    "save_dir = \"Pruned_Resnet\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Now we perform the training phase\n",
    "# Set hyperparameters\n",
    "num_epochs = 3\n",
    "fine_tune_epochs = 1\n",
    "learning_rate = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "regularizer = 1e-5\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96afa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    pruned_model.train()\n",
    "    train_loader_iter = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for idx, (images, _) in enumerate(train_loader_iter):\n",
    "        images = images.to(device)\n",
    "\n",
    "        true_outputs = original_model(images)\n",
    "        pred_outputs = pruned_model(images)\n",
    "\n",
    "        loss = criterion(pred_outputs, true_outputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        pruner.regularize(pruned_model, reg=regularizer)\n",
    "        train_loader_iter.set_postfix({'Loss': loss.item()})\n",
    "    torch.save(pruned_model, os.path.join(save_dir, 'pruned_model.pth'))\n",
    "        \n",
    "print(\"Pruned model saved.\")\n",
    "\n",
    "# Save the pruned model\n",
    "torch.save(pruned_model, os.path.join(save_dir, 'pruned_model.pth'))\n",
    "print(\"Pruned model saved.\")\n",
    "\n",
    "# Pruning and fine-tuning iteration\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(pruned_model, example_inputs)\n",
    "for i in range(iterative_steps):\n",
    "    pruner.step()\n",
    "\n",
    "    macs, nparams = tp.utils.count_ops_and_params(pruned_model, example_inputs)\n",
    "    print(pruned_model(example_inputs).shape)\n",
    "    print(\n",
    "        \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "        % (i+1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "    )\n",
    "    print(\n",
    "        \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "        % (i+1, iterative_steps, base_macs / 1e9, macs / 1e9)\n",
    "    )\n",
    "    print(\"=\"*16)\n",
    "    # Fine-tune your model here\n",
    "    for epoch_ft in range(fine_tune_epochs):\n",
    "        fine_tune_loader_iter = tqdm(fine_tune_loader, desc=f'Fine-tuning Epoch {epoch_ft + 1}/{fine_tune_epochs}', leave=False)\n",
    "        for images, _ in fine_tune_loader_iter:\n",
    "            images = images.to(device)\n",
    "\n",
    "            true_outputs = original_model(images)\n",
    "            pred_outputs = pruned_model(images)\n",
    "\n",
    "            loss = criterion(pred_outputs, true_outputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            fine_tune_loader_iter.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(pruned_model, os.path.join(save_dir, 'fine_tuned_model.pth'))\n",
    "print(\"Fine-tuned model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
